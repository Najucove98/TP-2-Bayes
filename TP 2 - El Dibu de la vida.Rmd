---
title: ""
author: ""
date: ""
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
    toc: false
    number_sections: false
    keep_tex: true
header-includes:
  - \renewcommand{\figurename}{Figura}
  - \renewcommand{\tablename}{Tabla}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{titling}
  - \usepackage{lipsum}
  - \usepackage{fancyhdr}
  - \usepackage{etoolbox}
  - \usepackage{setspace}
  - \usepackage{titlesec}
  - \usepackage{emptypage}
  - \pagestyle{fancy}
  - \usepackage{placeins}
  - \fancyhf{}
  - \patchcmd{\maketitle}{\@maketitle}{\centering\vspace*{4cm}\@maketitle}{}{}
  - \thispagestyle{empty}
editor_options: 
  markdown: 
    wrap: 72
---

```{=tex}
\begin{titlepage}
\centering
\vspace*{4cm} % espacio superior

{\Huge \textbf{Trabajo práctico 2 Estadística Bayesiana}}\\[2cm]

{\Large Agustina Roura}\\[0.5cm]
{\Large Cristian Nahuel Coveñas}\\[0.5cm]
{\Large Juan Sebastian Reines}\\[2cm]

{\large Fecha: Mayo 2025}

\vfill

\end{titlepage}
```
\newpage

# Introducción

El ozono $(O_3)$, una molécula de tres átomos de oxígeno, exhibe una
fascinante dualidad en su comportamiento y efectos. En las capas
superiores de la atmósfera, conforma la vital capa de ozono, un escudo
protector que filtra la radiación ultravioleta del sol, permitiendo la
vida tal como la conocemos. Sin embargo, a nivel de la superficie
terrestre, este mismo gas se transforma en un contaminante atmosférico
de considerable impacto negativo, afectando la salud humana a través de
la irritación del sistema respiratorio y ocular, el desencadenamiento de
episodios de asma y el deterioro de la función pulmonar. Además, su
presencia en el aire urbano contribuye al smog y puede causar daños
significativos en la vegetación.

Ante la creciente preocupación por los efectos del ozono a nivel del
suelo, Brian Tarkington, investigador del California Primate Research
Center, llevó a cabo un estudio con el objetivo de cuantificar su
impacto en el desarrollo de organismos vivos. Específicamente, se
propuso investigar si la exposición a un ambiente enriquecido con ozono
afectaba el crecimiento de ratas jóvenes. Para ello, diseñó un
experimento controlado donde un grupo de 46 ratas de la misma edad
fueron divididas aleatoriamente en dos subgrupos iguales. Uno de estos
grupos fue expuesto a un ambiente con alta concentración de ozono,
mientras que el otro se mantuvo en un ambiente libre de este gas. Tras
un período de siete días, se registró la diferencia en el peso de cada
animal, generando un conjunto de datos que permite analizar
comparativamente el efecto de la exposición al ozono.


Con el objetivo de dar respuesta a la inquietud de Tarkington, abordaremos 
el problema mediante el uso modelos bayesianos. En particular, consideramos
dos modelos probabilísticos con sutiles diferencias en su formulación.

El primer modelo propuesto para modelizar el cambio en el peso de las ratas 
en cada grupo se modeliza con una distribución normal, permitiendo medias
$(\mu_O,\ \mu_C)$ y varianzas $(\sigma_O^2,\ \sigma_C^2)$ específicas para 
cada grupo.

El segundo modelo propuesto, en contraste, emplea la distribución T
de Student con 3 grados de libertad $(v = 3)$, esta alternativa
resulta especialmente útil cuando se desea robustez frente a posibles
valores atípicos o colas pesadas en la distribución de los datos.

Más allá de la comparación de estos modelos y la respuesta a la pregunta
de investigación sobre el efecto del ozono, el objetivo principal del
estudio es profundizar el uso del algoritmo de Metropolis-Hastings (MH)
como un método de la inferencia bayesiana. Este algoritmo se vuelve
esencial cuando la distribución de probabilidad a posteriori de los
parámetros de interés no tiene una forma analítica conocida, lo que
dificulta la obtención directa de medidas de resúmenes o la realización
de inferencias. En esencia, el algoritmo de Metropolis-Hastings funciona
como un proceso iterativo que genera una secuencia de valores (una
cadena de Markov) que, bajo ciertas condiciones, converge a la
distribución a posteriori deseada. Partiendo de un valor inicial para
los parámetros, el algoritmo sugiere un nuevo valor $propuesto$ basado
en una distribución conocida. Este nuevo valor es luego aceptado o
rechazado según una probabilidad $\alpha$ que depende de la razón entre
la verosimilitud de los datos bajo el nuevo valor de los parámetros y la
verosimilitud bajo el valor actual, multiplicada por la razón de las
probabilidades a priori de los dos valores.

$$\alpha = \min \left\{ 1, \frac{p(y')q(y^{(t)} | y')}{p(y^{(t)})q(y' | y^{(t)})} \right\}$$

Si el valor $propuesto$ tiene una mayor probabilidad a posteriori (o una
probabilidad no mucho menor), es más probable que sea aceptado. Si es
rechazado, el valor actual se mantiene para la siguiente iteración. Tras
un número suficiente de iteraciones, las muestras generadas por el
algoritmo se aproxima a la distribución del $posterior$ objetivo y asi
calcular diversas cantidades de interés, como medias, medianas,
intervalos de credibilidad, etc. La clave del algoritmo radica en la
elección adecuada de la distribución de propuesta para asegurar una
exploración eficiente del espacio de parámetros y una rápida
convergencia a la distribución objetivo.

\newpage

## Índice

1.[**En el simulador: ensayos con la distribución Gamma**](#Objetivo-1)

2.[**Free practice: múltiples dimensiones**](#Objetivo-2)

3.[**El Gran Prix: ¿qué le decimos a Brian?**](#Objetivo-3)

\newpage

```{r , echo=FALSE, message=FALSE, warning=FALSE}
library(purrr)
library(latex2exp)
library(grid)
library(gridExtra)
library(tidyverse)
library(knitr)
library(kableExtra)

# Funciones estandar para graficar
graficar_distribucion <- function(x_muestras, x_grid, p_grid, xlab = "x" ,ylab = "y", color = "grey60", color_l = "#3b78b0") {
  # Graficar histograma de las muestras con la curva de densidad teórica superpuesta.
  plt <- ggplot() +
    geom_histogram(
      aes(x = x, y = after_stat(density)),
      fill = color,     #cambie
      alpha = 0.5,      #agregue
      color = "white",  #agregue
      linewidth = 0.1,  #agregue
      bins = 50,
      data = data.frame(x = x_muestras)
    ) +
    geom_line(
      aes(x = x, y = y),
      color = color_l,  #cambie
      linewidth = 0.8, #cambie
      data = data.frame(x = x_grid, y = p_grid)
    ) +
    labs(y = ylab,      #cambie
       x= xlab) +       #agregue
    theme_bw() +
    theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(), #agregue
      axis.ticks.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none"
    )
  return(plt)
}


graficar_trazas_md <- function(df_muestras, ylab = "x" , colors = c("#107591", "#00c0bf", "#f69a48", "#fdcd49") ) {
  # Generar un _traceplot_ para 4 cadena de Markov.
  plt <- ggplot(df_muestras) +
    geom_line(aes(x = paso, y = valor, group = cadena, color = cadena), linewidth = 0.2) +
    scale_color_manual(values = colors) +
    labs(x = "Paso", y = ylab) +
    theme_bw()
  theme(
    panel.grid.minor = element_blank()
  )
  
  return(plt)
}

graficar_traza <- plot_trace <- function(x_muestras, color = "grey30", ylab = "x") {
  # Generar un _traceplot_ para una cadena de Markov.
  plt <- data.frame(x = seq_along(x_muestras), y = x_muestras) |>
    ggplot() +
    geom_line(aes(x = x, y = y), color = color , alpha = 0.8, linewidth = 0.2) + #agregue alpha
    labs(x = "Paso", y = ylab) + #cambie
    theme_bw()
  theme(
    panel.grid.major = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    legend.position = "none"
  )
  return(plt)
}

# Traemos las funciones que vamos a utilizar para el método de Metropolis-Hastings
metropolis_hastings_normal <- function(n, x, p, sigma, verbose = FALSE) {
  # Algortimo de Metropolis Hastings en una dimensión.
  # La distribución de propuesta es normal.
  # Parámetros
  #  ------------------------------------------------------------------------
  #  n         -Cantidad de muestras a obtener.                             
  #  x         -Posición inicial del algoritmo.                             
  #  p         -Función de densidad objetivo, normalizada o sin normalizar. 
  #  sigma     -Desvío estándar de la distribución de propuesta normal.     
  #  verbose   -Indica si se muestran mensajes con información del          
  #             algoritmo en cada paso.                                     
  #  ------------------------------------------------------------------------
  
  muestras <- numeric(n)
  muestras[1] <- x
  
  # Iterar desde i = 1 hasta i = n - 1
  for (i in seq_len(n - 1)) {
    # Paso 1: Proponer un nuevo valor
    x_actual <- muestras[i]
    x_propuesto <- rnorm(1, mean = x_actual, sd = sigma)
    
    # Paso 2: Calcular probabilidad de aceptación
    p_actual <- p(x_actual)
    p_propuesto <- p(x_propuesto)
    
    # Corrección en base a la densidad de la distribución de propuesta
    q_propuesto <- dnorm(x_propuesto, mean = x_actual, sd = sigma)  # q(x_propuesto | x_actual)
    q_actual <- dnorm(x_actual, mean = x_propuesto, sd = sigma)     # q(x_actual | x_propuesto)
    
    alpha <- min(1, (p_propuesto / p_actual) * (q_actual / q_propuesto))
    
    # Paso 3: Dedicir si se acepta el valor propuesto
    u <- runif(1)
    aceptar <- u < alpha
    
    # Guardar posición
    if (aceptar) {
      muestras[i + 1] <- x_propuesto
    } else {
      muestras[i + 1] <- x_actual
    }
    
    # Si 'verbose' es TRUE, mostrar valores de variables relevantes
    if (verbose) {
      cat("-----------------------\n")
      cat("x_actual: ", x_actual, "x_propuesto", x_propuesto, "\n")
      cat("p_actual: ", p_actual, "p_propuesto", p_propuesto, "\n")
      cat("q_propuesto: ", q_propuesto, "q_actual", q_actual, "\n")
      cat("alpha:", alpha, "u:", u, "aceptar:", aceptar, "\n")
    }
  }
  
  return(muestras)
}

crear_df_muestras <- function(muestras) {
  # Muestras es una lista de vectores.
  # Hay tantos vectores como cadenas.
  
  n_cadenas <- length(muestras)
  n_muestras <- length(muestras[[1]])
  
  df <- data.frame(
    cadena = as.factor(rep(seq_len(n_cadenas), each = n_muestras)),
    paso = rep(seq_len(n_muestras), n_cadenas),
    valor = unlist(muestras)
  )
  return(df)
}

```

# Primeros pasos en Metropolis-Hastings {#Objetivo-1}

El algoritmo de Metropolis-Hastings, implementado en escala logarítmica,
constituye una herramienta robusta para obtener muestras del
$posterior$. Sin embargo, su aplicación directa a modelos complejos
puede ser desafiante sin una sólida comprensión de sus fundamentos y
técnicas asociadas. Como preparación para la aplicación del algoritmo de
Metropolis-Hastings a modelos bayesianos más complejos, se llevó a cabo 
una exploración inicial del algoritmo en escenarios más simples. 

Se comenzó con un escenario donde $X$ es la variable aleatoria
de interés, cuya función de densidad corresponde a una distribución
Gamma con parámetros $\alpha = 3$ y $\beta = 2$ ($X \sim Gamma(3,2)$).

```{r f1,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Distribucion de densidad de X" }
set.seed(0398)
# Cremos una grilla de valores de x:
# X está en los reales positivos
grid_x <- seq(0, 6, length.out = 500)

# Creamos la grilla de las p_x
grid_px <- dgamma(grid_x, 3, 2)

# Graficamos la funcion de densidad de la X
ggplot(data.frame(x = grid_x, y = grid_px), aes(x = x, y = y)) +
  geom_area(fill = "#3b78b0", alpha = 0.5) +
  geom_line(linewidth = 0.8, color = "#3b78b0") +
  labs(y = TeX("$f_X(x)$")) + 
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
  )
```

La Figura @ref(fig:f1) ilustra la forma de la distribución de densidad
de la variable aleatoria $X$.

A continuación, se propuso encontrar la función de densidad de una nueva
variable aleatoria $Y=log(X)$. Para ello, recurrimos a la técnica de
transformación de variables de la siguiente manera:

$\text{Sea }Y = g(X) = log(x) \text{, donde la función } g:(0,\ +\infty) \rightarrow \mathbb{R}.$

$\text{Su función inversa es: }g^{-1}(X) = e^{X} \text{, con dominio } g^{-1}: \mathbb{R} \rightarrow (0,\ +\infty).$

Luego, la función de densidad de $Y$, $P_Y(y)$, se obtiene a partir de
la función de densidad $X$, $P_X(x)$, utilizando la siguiente relación:
\vspace{0cm}
$$P_Y(y) = P_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$$
$\text{Sustituyendo } g^{-1}(y) = e^{y}, \text{ obtenemos:}$
\vspace{0cm}
$$P_Y(y) = P_X(e^{y}) \left| \frac{d}{dy} e^{y} \right| = P_X(e^{y})\left| e^{y} \right|$$

\newpage

$\text{De esta forma podemos graficar la función de densidad de } Y = log(X)$

```{r f2,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Distribucion de densidad de Y = log(X)"}
set.seed(0398)
# Creamos una funcion:
py <- function(y){
  dgamma(exp(y), shape = 3, rate = 2) * exp(y) 
}

# Creamos una grilla de valores de y:
# Y está en los reales
grid_y <- seq(-4, 3, length.out = 10000)

# Creamos la grilla de las p_y
grid_p_y <- py(grid_y)

# Graficamos la funcion de densidad de Y
ggplot(data.frame(x = grid_y, y = grid_p_y), aes(x = x, y = y))  +
  geom_area(fill = "#44A699", alpha = 0.5) +
  geom_line(linewidth = 0.8, color = "#44A699") +
  labs(y = TeX("$f_Y(y)$"),
       x= "y") + 
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
  )
```

Dadas las funciones de densidad para las variables $X$ e $Y=log(X)$, tal
como se ilustra en las Figuras @ref(fig:f1) y @ref(fig:f2)
respectivamente, se consideró la aplicación del algoritmo de
Metropolis-Hastings para generar 10000 muestras de $X$. Si bien la
distribución normal suele ser una elección inicial conveniente para
generar propuestas de nuevos valores en el algoritmo de
Metropolis-Hastings, su aplicación en espacios paramétricos acotados
puede resultar ineficiente debido al incremento en la tasa de rechazo
generado por propuestas fuera del dominio, lo que ralentiza la
exploración del espacio paramétrico y la convergencia del algoritmo.

Porlo que se optó por realizar el proceso de muestreo sobre la variable
transformada $Y=log(X)$, utilizando una distribución normal como función
de propuesta, con una desviación estándar $\sigma = 0.2$ y centrada en
un valor inicial de $\mu = 0$. Esta estrategia de trabajar en un espacio
no acotado permitió seguir utilizando las propiedades convenientes de la
distribución normal como propuesta.

```{r f3,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de Y = log(X)"}
set.seed(0398)
# Sacamos muestras:
# Hacemos un nuevo grid_py:
grid_y <- seq(-4, 3, length.out = 10000)

muestra <- metropolis_hastings_normal(
  n = 10000,
  x = 0,
  sigma = 0.2,
  p = py
  )

# Graficando:
graficar_distribucion(muestra, 
                      # En el eje x van los grid_py(osea los valores 
                      # "simulados" de y)
                      x_grid = grid_y, 
                      # En el eje y van los valores de P(y) 
                      p_grid = py(grid_y),
                      xlab = "y",
                      ylab = TeX("$f_Y(y)$"),
                      color = "#44A699",
                      color_l = "#44A699")
```

```{r f4,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplot de la muestras de Y = log(X)"}
graficar_traza(muestra, ylab = "y", color = "#44A699" )
```

Luego de la aplicación del método, como se puede observar en la Figura
@ref(fig:f3), las muestras obtenidas presentan una semejanza notable con
la distribución objetivo. Además como se puede observar en la Figura 
@ref(fig:f4) (trace plot) revela una baja autocorrelación en las cadenas 
de Markov generadas. Esto sugiere un buen ajuste del algoritmo de 
Metropolis-Hastings, resultado de su aplicación sobre la variable aleatoria Y, 
cuyo dominio resultó apropiado para la distribución de propuesta utilizada.

Para una evaluación más exhaustiva de la calidad del ajuste del método 
implementado, se calcularon las medidas de diagnóstico sobre la distribución 
propuesta utilizada para generar la muestra.

```{r,echo=FALSE, warning=FALSE, message=FALSE}

# Funciones para medidas de diagnostico
# Diagnóstico R de Gelman-Rubin
calcular_W <- function(muestras) {
    # muestras: matriz de dimensión (n_muestras, n_cadenas)
    return(mean(apply(muestras, 2, var)))
}

calcular_B <- function(muestras) {
    # muestras: matriz de dimensión (n_muestras, n_cadenas)
    S <- nrow(muestras) # cantidad de muestras
    M <- ncol(muestras) # cantidad de cadenas
    media_cadenas <- apply(muestras, 2, mean)
    media_global <- mean(media_cadenas)
    diferencias_cuadrado <- (media_cadenas - media_global) ^ 2
    return(S / (M - 1) * sum(diferencias_cuadrado))
}

calcular_R <- function(muestras) {
    # muestras: matriz de dimensión (n_muestras, n_cadenas)
    S <- nrow(muestras)
    W <- calcular_W(muestras)
    B <- calcular_B(muestras)
    numerador <- ((S - 1) / S) * W + B / S
    return(sqrt(numerador / W))
}

# Tamaño efectivo de muestra N_eff
calcular_n_eff <- function(muestras) {
    # muestras: vector de longitud 'n_muestras'
    autocorrelacion <- acf(muestras, lag = Inf, plot = FALSE)$acf
    limite <- which(autocorrelacion < 0.05)[1]
    numerador <- length(muestras)
    denominador <- 1 + 2 * sum(autocorrelacion[2:limite])
    return(numerador / denominador)
}

```


```{r f5,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplots con sigma = 0.2"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas de muestras con el metodo usado y una propuesta normal con sigma 0.2
muestras_md <- list()
n_muestras <- 10000
n_cadenas <- 4
x_inicio <- 0
sigma <- 0.2

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md[[j]] <- metropolis_hastings_normal(
    n = n_muestras,
    x = x_inicio,
    p = py,
    sigma = sigma
  )
}

#Con esto descartamos las primeras mil muestras que aun "no convergen"
#df_muestras <- crear_df_muestras(muestras) |> filter(paso > 1000)


#necesario para crear el traceplot
df_muestras_md <- crear_df_muestras(muestras_md)

# Grafica las 4 trazas
graficar_trazas_md(df_muestras_md, ylab = "y", colors = c("#AEC6CF", "#F08080", "#FAEBD7", "#DDA0DD"))

```




```{r t1,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center",  fig.width = 4, fig.height = 2.8}
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_md$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")

```


El análisis visual del trace plot, presentado en la Figura @ref(fig:f5), sugiere una convergencia de las cuatro cadenas de Markov hacia una distribución común, lo que es consistente con el valor de $\hat{R}$ de 1.001601, cercano al valor ideal de 1 que indica una buena mezcla entre las cadenas. No obstante, al examinar el tamaño efectivo de muestra ($N_{eff}$) presentado en la Tabla 1, se observa que los valores para cada cadena y el total son considerablemente bajos en relación con el número total de iteraciones. Este reducido tamaño efectivo de muestra indica una alta autocorrelación dentro de las cadenas. En consecuencia, la alta autocorrelación compromete la validez de utilizar directamente estas muestras para realizar inferencias precisas sobre la distribución objetivo.


Se continuó el análisis realizando el mismo procedimiento evaluando los resultados de las medidas de diagnóstico $\hat{R}$ y $N_{eff}$ , a través de diferentes valores de sigma.


```{r ,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplots con σ = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas de muestras con el metodo usado y una propuesta normal con sigma 0.2
muestras_md <- list()
n_muestras <- 10000
n_cadenas <- 4
x_inicio <- 0
sigma <- 1.3

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md[[j]] <- metropolis_hastings_normal(
    n = n_muestras,
    x = x_inicio,
    p = py,
    sigma = sigma
  )
}

# Con esto descartamos las primeras mil muestras que aun "no convergen"
df_muestras_md <- crear_df_muestras(muestras_md)
```


```{r f6,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplots con σ = 1.3"}
# Grafica las 4 trazas
traceplot2 <- graficar_trazas_md(df_muestras_md, ylab = "y", colors = c("#87CEEB", "#FFA07A", "#FFFFE0", "#B0E0E6"))

traceplot2
```

```{r t2,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_md$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas


# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")

```



```{r ,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE}
set.seed(2198)
# Copiando lo del profe:
muestras <- metropolis_hastings_normal(
  n = 10000,
  x = 0,
  p = py ,
  sigma = 1.3
)

```


Por lo que tomando en consideración dicho análisis se concluyó, como se puede observar en la Figura @ref(fig:f6), que el valor de $\sigma = 1.3$ parece ser el más adecuado para realizar inferencias ya que su valor de $\hat{R}$ fue el más cercano a 1 de los comparados y las muestras de las diferentes cadenas se mezclas y parecen generar muestras representativas de la distribución objetivo ya que se estabilizan y muestran una buena mezcla. Además, el tamaño efectivo de muestra total fue el mayor, lo que señala que la autocorrelación es baja, y la menor de todas las comparadas. Por lo que se decidió continuar el análisis con un valor de $\sigma = 1.3$ para la distribución propuesta. También se consideró apropiado no eliminar las primeras 1000 muestras ya que las cadenas de markov parecieran converger rápidamente a la misma distribución, lo que indica una elección apropiada del valor inicial.


Se implementó entonces la distribución propuesta normal centrada en 0, con $\sigma = 1.3$, para obtener muestras de la variable X por medio del algoritmo Metropolis Hastings.



```{r f7,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de X"}
# Transformamos las muestras de Y a muestras de X:
muestras_x <- exp(muestras)

# histograma de la muestra objetivo
graficar_distribucion(muestras_x, grid_x, grid_px , ylab = TeX("$f_X(x)$"), color = "#3b78b0", color_l = "#3b78b0")
```

```{r f8,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplot de la muestras de X"}
# Traza de la muestra objetivo
graficar_traza(muestras_x, color = "#3b78b0")
```


Como se puede observar en la Figura @ref(fig:f7), la muestras obtenidas del $posterior$ con una distribución propuesta sugieren una buena aproximacion de la densidad teorica. Adicionalmente, en la @ref(fig:8),  se muestra un patrón de ruido blanco, lo que parece señalar una pérdida bastante rápida de la dependencia de cada muestra con la anterior. En resumen, las muestras tienen menos autocorrelación.


## Escala logarítmica 

La implementación efectiva del algoritmo de Metropolis-Hastings, se enfrenta a desafíos computacionales relacionados con la magnitud de los números involucrados. Un obstáculo surge al evaluar la función de densidad objetivo en cada iteración del algoritmo. Esta función, que guía el proceso de muestreo hacia las regiones de mayor probabilidad, puede implicar la multiplicación de numerosos términos, especialmente cuando se modelan datos con múltiples observaciones. Si estos términos son pequeños, su producto puede rápidamente volverse tan pequeño que la computadora lo representa como cero, es decir se produce el  fenómeno conocido como underflow. Este fenómeno dificulta la correcta ejecución del algoritmo de Metropolis-Hastings ya que puede conducir a errores que interrumpen el proceso de muestreo, impidiendo la obtención de las muestras necesarias para realizar inferencias válidas. Incluso si el programa no se detiene, un underflow puede resultar en la aceptación o rechazo incorrecto de valores propuestos, desviando la cadena de Markov de la distribución objetivo real y disminuyendo la fiabilidad de los resultados. Además, como el algoritmo evalúa la densidad objetivo en diferentes puntos del espacio paramétrico, la probabilidad de encontrar estas situaciones problemáticas se multiplica.


Una solución práctica es el uso de la escala logarítmica, al trabajar con los logaritmos de las probabilidades en lugar de las probabilidades directas, las operaciones de multiplicación se convierten en sumas y las divisiones se transforman en restas. Lo cual evita la utilización de números extremadamente pequeños, estabilizando los cálculos dentro del algoritmo de Metropolis-Hastings. 

\newpage

A continuación, se implementó con el método de metropolis hastings en escala logarítmica la obtención de muestras de la variable X. 


```{r,echo=FALSE, warning=FALSE, message=FALSE}
# Completamos el codigo:
metropolis_hastings_log <- function(logp, x, n, sigma = NULL) {
  # Algortimo de Metropolis Hastings en escala logarítmica
  #
  # Parámetros
  #  ------------------------------------------------------------------------
  #  logp      Función de densidad objetivo, normalizada o sin normalizar, |
  #            en escala logarítmica.                                      |
  #  x         Posición inicial del algoritmo.                             |
  #  n         Cantidad de muestras a obtener.                             |
  #  sigma     Matriz de varianzas y covarianzas para la distribución de   |
  #            propuesta. Por defecto, es NULL y usa la matriz identidad.  |
  #  ------------------------------------------------------------------------
  
  # Obtener dimensionalidad de la distribución objetivo a partir del punto inicial
  p <- length(x)
  
  # Inicializar matriz donde se guardan las muestras
  muestras <- matrix(NA, nrow = n, ncol = p)
  
  # Usar matriz diagonal unitaria para la distribución de propuesta cuando no se especifica
  if (is.null(sigma)) {
    sigma <- diag(p)
  }
  
  # Almacenar el punto inicial en la matriz de muestras
  muestras[1, ] <- x
  
  for (i in 1:(n - 1)) {
    # Obtener el valor de la muestra actual
    muestra_actual <- muestras[i, ]
    
    # Proponer un nuevo valor
    muestra_propuesta <- mvtnorm::rmvnorm(1, mean = muestra_actual, sigma = sigma)
    
    # Evaluar la log densidad en el valor actual y en el propuesto
    logp_propuesta <- logp(muestra_propuesta)
    logp_actual <- logp(muestra_actual)
    
    # Log densidad al pasar de muestra_propuesta a muestra_actual y viceversa
    logq_actual <- mvtnorm::dmvnorm(
      muestra_actual, mean = muestra_propuesta, sigma = sigma, log = TRUE
    )
    logq_propuesta <- mvtnorm::dmvnorm(
      muestra_propuesta, mean = muestra_actual, sigma = sigma, log = TRUE
    )
    
    # Calcular log probabilidad de aceptación
    # Es mas estable con sumas y restas
    log_alpha <- (
      (logp_propuesta - logp_actual) + (logq_actual - logq_propuesta)
    )
    
    # Simular aceptación o rechazo
    # Opcion 1: comparacion en escala logarítmica
    log_u <- log(runif(1))
    
    aceptar <- log_u < log_alpha
    
    # Opcion 2: comparacion en escala original
    # u <- runif(1)
    # aceptar <- u < exp(log_alpha)
    
    # Determinar siguiente paso en base al criterio de selección
    if (aceptar) {
      muestras[i + 1, ] <- muestra_propuesta
    } else {
      muestras[i + 1, ] <- muestra_actual
    }
  }
  # Convertir 'muestras' a vector si se trata de una distrbución univariada
  if (p == 1) {
    muestras <- as.vector(muestras)
  }
  
  return(muestras)
}
```


```{r,echo=FALSE, warning=FALSE, message=FALSE}
set.seed(2198)
# Funcion en escala logaritmica
logp_y <- function(y){
  # Equivalente a log(dgamma(x, shape = 3, rate = 2))
  # Pero es más estable:
  dgamma(exp(y), shape = 3, rate = 2, log = T) + y
}


muestralog <- metropolis_hastings_log(
  logp = logp_y,
  x = 0, 
  n = 10000, 
  # Como usamos la normal multivariada, no usamos desvios o variancias
  # usamos la matriz de covariancias, pero en los elementos diagonales
  # tiene las variancias, no los desvíos, por eso se eleva al cuadrado
  sigma = as.matrix(1.3^2)
)
```

```{r f9,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de X"}
# Muestras de X transformando la muestra de Y
muestras_x <- exp(muestralog)
graficar_distribucion(muestras_x, grid_x, grid_px, ylab = TeX("$f_X(x)$"), color = "#3b78b0", color_l = "#3b78b0")

```


```{r f10,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplot de la muestras de Y"}
graficar_traza(muestras_x, color = "#3b78b0")                           
```



```{r f11,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplots con sigma = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas de muestras con el metodo usado y una propuesta normal con sigma 0.2
muestras_md_log <- list()
n_muestras <- 10000
n_cadenas <- 4
x_inicio <- 0
sigma <- as.matrix(1.3^2)

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md_log[[j]] <- metropolis_hastings_log(
    n = n_muestras,
    x = x_inicio,
    logp = logp_y,
    sigma = sigma
  )
}

# Con esto descartamos las primeras mil muestras que aun "no convergen"
df_muestras_md_log <- crear_df_muestras(muestras_md_log)

# Grafica las 4 trazas
graficar_trazas_md(df_muestras_md_log, ylab = "x", colors = c("#87CEEB", "#FFA07A", "#FFFFE0", "#B0E0E6"))
```

```{r t3,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_md_log$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)


# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")


```


La Figura @ref(fig:f9) muestra el histograma de las muestras obtenidas para la variable X, indicando una buena cobertura de su rango de variabilidad. El traceplot correspondiente a la Figura @ref(fig:f11) sugiere convergencia de las cadenas de markov hacia una misma distribución. Es notable que las medidas de diagnóstico ($\hat{R}$ y $N_{eff}$) para estas muestras, mostradas en la Tabla 3, son idénticas a las calculadas previamente para la variable Y (Tabla 2 de la @ref(fig:f6)). Esta coincidencia sucede ya que, el algoritmo de Metropolis-Hastings se implementó utilizando cálculos en escala logarítmica para la estabilidad numérica para su optimización, las muestras generadas de la variable X se obtienen a partir del mismo valor de inicio, y por lo tanto sus propiedades de convergencia y eficiencia evaluadas por $\hat{R}$ y $N_{eff}$, permanecen inalteradas.




\newpage

# Inferencia Bayesiana en Múltiples Dimensiones {#Objetivo-2}

En modelos bayesianos con múltiples parámetros, la distribución a posteriori conjunta se vuelve el objetivo de inferencia. En este caso, la distribución objetivo tiene múltiples dimensiones y se obtiene mediante la aplicación de la regla de Bayes.

### Modelo Estadístico:
\vspace{-1.6cm}
\begin{center}
\begin{align*}
Y_i &\sim \text{Normal}(\mu, \sigma^2), \text{ con } i = 1, \dots, N \\
\mu &\sim \text{Normal}(0, 1^2) \\
\sigma &\sim \text{Gamma}(\alpha = 2, \beta = 2)
\end{align*}
\end{center}

La distribución a posteriori conjunta de los parámetros $\theta = \{\mu, \sigma\}$ dado los datos $y$ se obtiene mediante la regla de Bayes:
$$p(\mu, \sigma | y) = \frac{p(y | \mu, \sigma). p(\mu, \sigma)}{p(y)} \propto p( \boldsymbol{y}  \text{ | } \mu, \ \sigma).p(\mu, \ \sigma)$$
donde $p(y | \mu, \sigma)$ es la verosimilitud de los datos dado los parámetros, $p(\mu, \sigma) = p(\mu) p(\sigma)$ es la distribución a priori conjunta de los parámetros (asumiendo independencia entre $\mu$ y $\sigma$).

La distribución a priori conjunta es el producto de las distribuciones a priori individuales:
$$p(\mu, \sigma) = p(\mu) . p(\sigma)$$

### Transformación a Espacio No Acotado

Un posible problema para la deducción analítica del posterior es el dominio de $\sigma$ ($\mathbb{R}^+$). Para facilitar el muestreo o la optimización, a menudo se realiza una transformación a un espacio no acotado. En este caso, se propone la transformación $\theta^* = \{\mu, \log(\sigma)\} \in \mathbb{R} \times \mathbb{R}$.

La relación entre $\sigma$ y $\sigma^*$ es $\sigma = \exp(\sigma^*)$

### Función de Densidad del Posterior Transformado

La función de densidad del posterior en el espacio transformado resulta:

$$p(\mu, \sigma^* \mid y) \propto p_y(y \mid \mu, \sigma^*).p_\mu(\mu).p_\sigma^*(\sigma^*)$$
$$\text{Teniendo en cuando la transformacion de }\sigma^* \text{ tenemos:}$$
$$p_\sigma^*(\sigma^*) = p_\sigma^*(g^1 (\sigma^*)) \left| \dfrac{d}{d\sigma^*} (g^1 (\sigma^*)) \right| \ = \ p_\sigma^*(exp(\sigma^*)).\exp(\sigma^*) $$
$$\text{De esta forma la función de densidad del posterior trasnformado resulta:}$$
$$p(\mu, \sigma^* \mid y) \propto\left[ \prod_{i = 1}^{N} p_y(y_i \mid \mu,\exp(\sigma^*))\right].p_\mu(\mu).p_\sigma^*(exp(\sigma^*)).\exp(\sigma^*)$$
$$\text{Aplicando logarítmo resulta:}$$
$$log\ p(\mu, \sigma^* \mid y) \propto\left[ \sum_{i = 1}^{N} log\ p_y(y_i \mid \mu,\exp(\sigma^*))\right]+log\ p_\mu(\mu)+log\ p_\sigma^*(exp(\sigma^*))+ \sigma^*$$

\newpage

### Implementación en R

```{r}
# Función de densidad posterior en escala logarítmica (sin normalizar)
log_posterior <- function(params, y_vector) {
  # 'params' tiene longitud 2
  mu <- params[1]
  sigma_estrella <- params[2]
  sigma <- exp(sigma_estrella) # Salida sigma debe ser positiva

  # Log-verosimilitud (asumiendo datos independientes)
  log_likelihood <- sum(dnorm(y_vector, mean = mu, sd = sigma, log = TRUE))

  # Log-prior de mu
  log_prior_mu <- dnorm(mu, mean = 0, sd = 1, log = TRUE)

  # Log-prior de sigma (transformado) - Incluyendo el Jacobiano
  log_prior_sigma <- dgamma(sigma, shape = 2, rate = 2, log = TRUE) + sigma_estrella

  # Log-posterior no normalizado
  log_posterior_value <- log_likelihood + log_prior_mu + log_prior_sigma

  return(log_posterior_value)
}
```

Inicialmente, se corrieron dos cadenas de Markov, una para el parámetro $\mu$ y otra para el parámetro $\sigma$, y se evaluaron sus medidas de diagnóstico como el tamaño efectivo de muestra y el estadístico $\hat{R}$, que indica la convergencia de las cadenas de markov. Sin embargo, los resultados preliminares sugirieron la necesidad de explorar diferentes combinaciones para el vector de valores iniciales y la matriz de variancia y covariancia $\sum$ de la distribución propuesta, buscando optimizar la calidad de las muestras obtenidas. 

```{r,echo=FALSE, warning=FALSE, message=FALSE}
# Utilizamos la muestra provista
y_datos <- as.numeric(readLines(
  "https://raw.githubusercontent.com/ee-unr/estadistica-bayesiana/refs/heads/main/datos/precalentamiento-mh.txt"))

# Se utiliza la funcion "partial" que permite pre-llenar argumentos
logp <- partial(log_posterior, y_vector = y_datos)
```


```{r,echo=FALSE, warning=FALSE, message=FALSE}
set.seed(0398)
# matriz de varianza y covarianza
sigma <- matrix(c(0.12^2, 0, 0, 0.13^2), nrow = 2)  # Propuesto en clase 0.05^2


# Obtener muestras
muestras_theta_estrella <- metropolis_hastings_log(
  logp = logp,
  x = c(1, log(0.8)), # propuesto en clase 0 y log(1.5)
  n = 10000,
  sigma = sigma
)
```


```{r f12,echo=FALSE, message=FALSE, fig.align = "center",  fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de mu"}
#histograma que muestra la dist a posteriori marginal de mu
graficar_distribucion(
  muestras_theta_estrella[ ,1], #[1000:10000]
  x_grid = 1, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 1,
  color = "green", #cambiar gama de colores para los parametros mu y sigma
  xlab = "μ",
  ylab = NULL)
# es un histograma que representa la distribucion aposteriori marginal de mu
```

```{r ,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplots con sigma = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas del vector de parametros
muestras_md_log <- list()
n_muestras <- 10000
n_cadenas <- 4
x_inicio <- 0
sigma <- matrix(c(0.12^2, 0, 0, 0.13^2), nrow = 2)

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md_log[[j]] <- metropolis_hastings_log(
    n = n_muestras,
    x = c(1, log(0.8)),  # Dos puntos iniciales
    logp = logp,
    sigma = sigma # matriz de 2x2 diagonal
  )
}

# creamos un data frame para poder graficar
df_muestras_md_log <- crear_df_muestras(muestras_md_log)

#dataframe con las muestras de mu
df_muestras_mu <- df_muestras_md_log |> filter(paso <= 10000)


#dataframe con las muestras de sigma 
df_muestras_sigma <- df_muestras_md_log |> filter(paso > 10000)

```


```{r f13,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplots con sigma = 1.3"}

# Grafica las 4 trazas
graficar_trazas_md(df_muestras_mu, ylab = "mu", colors = c("#87CEEB", "#FFA07A", "#FFFFE0", "#B0E0E6"))
```


```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}


set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_mu$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```

Las muestras definitivas del $posterior$ de $\mu$, obtenidas con una propuesta de desvío de 0.12 y un valor inicial de 1 mostradas en la Figura @ref(fig:f12), presentan buenas medidas de diagnóstico. La Figura @ref(fig:f12) muestra una adecuada mezcla de las cadenas de Markov, convergiendo a una misma distribución, lo que se confirma, en al Tabla 4,  con un valor de $\hat{R}$ aproximado a 1. El tamaño efectivo de muestra resultante fue de 5072.55, indicando una baja autocorrelación en comparación con otras pruebas.


```{r f14,echo=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de sigma"}
#histograma que muestra la dist a posteriori marginal de sigma (transformamos sigma_estrella)
graficar_distribucion(
  exp(muestras_theta_estrella[ ,2]), #[1000:10000]burn-in (los quema a los primeros 1000 pasos)
  x_grid = 1, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 1,  #igual que arriba
  color = "orange", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$σ$"),
  ylab = NULL)
```


```{r f15,echo=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Traceplots con sigma = 1.3"}
# Grafica las 4 trazas
df_muestras_sigma_trp <- df_muestras_sigma
df_muestras_sigma_trp[ ,3] <- exp(df_muestras_sigma[ ,3])
graficar_trazas_md(df_muestras_sigma_trp, ylab = TeX("$σ$"), colors = c("#87CEEB", "#FFA07A", "#FFFFE0", "#B0E0E6"))
```

```{r t4,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_sigma_trp$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```



En cuanto a $\sigma$, las muestras definitivas del $posterior$ mostrado en la Figura @ref(fig:f14), obtenidas con una propuesta de desvío de 0.13 y un valor inicial de 0.8, exhiben un $\hat{R}$ cercano a 1, lo que indica convergencia de las cadenas de markov  hacia una misma distribución, lo está corroborado visualmente en la Figura @ref(fig:f15). El tamaño efectivo de muestra para $\sigma$ fue de 6086.86, el más alto entre las propuestas comparadas.


\newpage


# Análisis exploratorio

Con el objetivo de comprender las características generales del conjunto de datos, se llevó a cabo un análisis exploratorio inicial. Con el fin de obtener una vision preliminar de los datos.

```{r,echo=FALSE, warning=FALSE, message=FALSE}
datos <- read.csv("tarkington.csv")

# Datos de control
datos_control <- datos %>% 
  filter(group == "Control")


# Crear un data frame para la tabla
tabla_medidas_control <- data.frame(
  Medida = c("$Media$","$Mediana$", "$Desvio$  $estandar$", "$Minimo$", "$Maximo$" ),
  Valor = c(mean(datos_control$gain), median(datos_control$gain), sd(datos_control$gain), 
            min(datos_control$gain), max(datos_control$gain))
)
```


```{r t5,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_medidas_control,
      col.names = c("Medida", "Valor"),
      caption = "Medidas descriptivas para datos de control",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")

```


```{r,echo=FALSE, warning=FALSE, message=FALSE}
# Datos de Ozone
datos_oz <- datos %>% 
  filter(group == "Ozone")


# Crear un data frame para la tabla
tabla_medidas_oz <- data.frame(
  Medida = c("$Media$","$Mediana$", "$Desvio$  $estandar$", "$Minimo$", "$Maximo$" ),
  Valor = c(mean(datos_oz$gain), median(datos_oz$gain), sd(datos_oz$gain), 
            min(datos_oz$gain), max(datos_oz$gain))
)
```


```{r t6,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_medidas_oz,
      col.names = c("Medida", "Valor"),
      caption = "Medidas descriptivas para datos de ozone",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")

```


```{r,echo=FALSE, warning=FALSE, message=FALSE}
# Graficos descriptivos
plot_con_hist <- ggplot(datos_control, aes(x = gain))+
  geom_histogram(
    bins = 22,
    fill = "#87CEEB",
    color = "grey20",
    linewidth = 0.1)+
  labs(y = "Frecuencia")+
  theme_bw() +
  theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(), #agregue
      axis.ticks.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none"
    )

plot_con_box <-ggplot(datos_control, aes(y = gain))+
  geom_boxplot(
    fill = "#87CEEB",
    linewidth = 0.1
  )+
  labs(y = "Gain")+
  theme_bw() +
  theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(), #agregue
      axis.ticks.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none"
    )

# Graficos descriptivos
plot_oz_hist <-ggplot(datos_oz, aes(x = gain))+
  geom_histogram(
    bins = 22,
    fill = "#FFA07A",
    color = "grey20",
    linewidth = 0.1)+
  labs(y = "Frecuencia")+
  theme_bw() +
  theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(), #agregue
      axis.ticks.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none"
    )

plot_oz_box <-ggplot(datos_oz, aes(y = gain))+
  geom_boxplot(
    fill = "#FFA07A",
    linewidth = 0.1)+
  labs(y = "Gain")+
  theme_bw() +
  theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(), #agregue
      axis.ticks.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none"
    )
```


```{r f16,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 7, fig.height = 3.5, fig.cap="Histograma y Boxplot para Control"}
grid.arrange(plot_con_hist, plot_con_box, ncol = 2)

```

```{r f17,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 7, fig.height = 3.5, fig.cap="Histograma y Boxplot para Ozono"}
grid.arrange(plot_oz_hist, plot_oz_box, ncol = 2)
```


En base a las medidas descriptivas obtenidas en las Tablas 5 y 6, se observa que el rango de ambas muestras parecen ser similares cubriendo valores desde -16.9 hasta un valor de 54.6.

El cambio de peso medio en las ratas expuestas a ozono representa aproximadamente la mitad del registrado en las ratas del grupo control, lo que indica una reducción en la ganancia de peso que podría ser atribuida  a la exposición al ozono, cabe destacar que la media es sensible a valores extremos por lo que no podemos sacar conclusiones de esta. Asimismo, el análisis mediante diagramas de caja (boxplots) revela que la distribución del cambio de peso en las ratas del grupo ozono presenta una asimetría hacia la izquierda, lo cual sugiere la presencia de valores más bajos en comparación con la mediana. En contraste, el cambio de peso en las ratas del grupo control muestra una distribución más concentrada, lo que indica una mayor homogeneidad en los datos de ese grupo.

\newpage


# Diferencias entre la distribución normal y la t de Student con 3 grados de libertad:

Si bien tanto la distribución normal como la t de Student con 3 grados de libertad, al compartir media y varianza, exhiben simetría alrededor de su valor central. La distribución normal mostró un decrecimiento mayor a medida que se aleja de la media, lo que implica una baja probabilidad de observar valores extremos. En contraste, la distribución t de Student presentó colas más pesadas, asignando una probabilidad significativamente mayor a la ocurrencia de valores atípicos en comparación con la normal.


En conclusión, en contextos caracterizados por muestras de tamaño reducido o una elevada incertidumbre sobre la varianza poblacional, la distribución t de Student se eligió como una opción preferible a la normal. Sus colas pesadas proporcionaron una mejor capacidad para modelar la variabilidad de cuando las muestras son más pequeñas, otorgando una mayor probabilidad a los valores atípicos y en consecuencia la distribución no es tan sensible a ellos.


```{r, echo = FALSE, warning=FALSE, message=FALSE}
# setear datos
prueba <- data.frame(
  valores = seq(-5, 5, length.out = 1000))

# agregar densidad
prueba$den_norm <- dnorm(prueba$valores)

scaled_t_density <- function(x, df, mean = 0, sd = 1) {
  dt((x - mean) / sd, df) / sd
}

prueba$den_t <- scaled_t_density(prueba$valores, 3)
```


```{r, echo = FALSE, warning=FALSE, message=FALSE}
# Graficar la curva de densidad
ggplot(prueba, aes(x = valores, y = den_norm)) +
  geom_line(aes(y = den_norm, color = "Normal")) +
  geom_line(aes(y = den_t, color = "T-Student (df = 3)")) +
  labs(
    title = "Comparación de densidades: Normal vs t-Student (df=3)",
    x = "valores", y = "Densidad", color = "Distribución") +
  theme_bw() +
  theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(), #agregue
      axis.ticks.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none"
    )
  
    
```


\newpage


# Comparación del modelo normal con el T de student

funcion del posterior generalizado en base logaritmica para eficiencia "computacional"
aca iria una introduccion y la formula del posterior para cada modelo

La distribución a posteriori conjunta de los parámetros $\theta = \{\mu_o,\mu_c, \sigma_o, \sigma_c\}$ dado los datos $y$ se obtiene mediante la regla de Bayes:
  $$p(\mu_o,\mu_c, \sigma_o, \sigma_c\ | \boldsymbol{y}) = \frac{p(\boldsymbol{y} | \mu_o,\mu_c, \sigma_o, \sigma_c). p(\mu_o,\mu_c, \sigma_o, \sigma_c)}{p(\boldsymbol{y})} \propto p( \boldsymbol{y}  \text{ | } \mu_o,\mu_c, \sigma_o, \sigma_c).p(\mu_o,\mu_c, \sigma_o, \sigma_c)$$
    donde $p_y(\boldsymbol{y} | \mu_o,\mu_c, \sigma_o, \sigma_c)$ es la verosimilitud de los datos dado los parámetros, $\\ p(\mu_o,\mu_c, \sigma_o, \sigma_c) = p_{\mu_o}(\mu_o).p_{\mu_c}(\mu_c).p_{\sigma_o}(\sigma_o) p_{\sigma_c}(\sigma_c)$ es la distribución a priori conjunta de los parámetros (asumiendo independencia entre $\mu_c , \mu_o$ y $\sigma_c , \sigma_o$).
    
La distribución a priori conjunta es el producto de las distribuciones a priori individuales:
$$p(\mu_o,\mu_c, \sigma_o, \sigma_c) = p_{\mu_o}(\mu_o).p_{\mu_c}(\mu_c).p_{\sigma_o}(\sigma_o) p_{\sigma_c}(\sigma_c)$$
      
### Transformación a Espacio No Acotado
      
Un posible problema para la deducción analítica del posterior es el dominio de $\sigma$ ($\mathbb{R}^+$). Para facilitar el muestreo o la optimización, a menudo se realiza una transformación a un espacio no acotado. En este caso, se propone la transformación $\theta^* = \{\mu_o,\mu_c, \sigma_o, \sigma_c\} \in \mathbb{R} \times \mathbb{R} \times \mathbb{R^+} \times \mathbb{R^+}$.
    
La relación entre $\sigma_o, \sigma_c$ y $\sigma_o^*, \sigma_c^*$ es $\sigma_0 = \exp(\sigma_o^*) \ \sigma_c = \exp(\sigma_c^*)$
      
### Función de Densidad del Posterior Transformado
      
La función de densidad del posterior en el espacio transformado resulta:
      
$$p(\mu_o,\mu_c, \sigma_o^*, \sigma_c^*\ | \boldsymbol{y}) \propto p( \boldsymbol{y}  \text{ | } \mu_o,\mu_c, \sigma_o, \sigma_c).p_{\mu_o}(\mu_o).p_{\mu_c}(\mu_c).p_{\sigma_o^*}(\sigma_o^*) p_{\sigma_c^*}(\sigma_c^*)$$

$$\text{Teniendo en cuando la transformacion de }\sigma_o^*, \sigma_c^* \text{ tenemos:}$$

$$p_\sigma^*(exp(\sigma_o^*),exp(\sigma_c^*))*\exp(\sigma_o^*)*exp(\sigma_c^*) $$

$$\text{De esta forma la función de densidad del posterior trasnformado resulta:}$$
$$p(\mu_o,\mu_c, \sigma_o^*, \sigma_c^*\ | \boldsymbol{y}) \propto\left[ \sum_{i = 1}^{N} p_y(y_i \mid \mu_o, \mu_c,\exp(\sigma_o^*)*exp(\sigma_c^*))\right] + \log{p_{\mu_o}(\mu_o)} + p_{\sigma_o}^*(exp(\sigma_o^*)) + \sigma_o^* + \log{p_{\mu_c}(\mu_c)} + p_{\sigma_c}^*(exp(\sigma_c^*)) + \sigma_c^*$$
      
      

```{r}
# Función de densidad posterior en escala logarítmica (Ozono)
log_posterior_oz <- function(params, y_vector, normal = TRUE) {
  # 'params' tiene longitud 4
  mu_oz <- params[1]
  sigma_estrella_oz <- params[2]
  sigma_oz <- exp(sigma_estrella_oz) # Salida sigma debe ser positiva
  
  if(normal == T){
    
    # Log-verosimilitud (asumiendo datos independientes)
    log_likelihood_oz <- sum(dnorm(y_vector, mean = mu_oz, sd = sigma_oz, log = TRUE))
    
  } else {
    # Log-verosimilitud (asumiendo datos independientes)
    log_likelihood_oz <- sum(log(scaled_t_density(y_vector, df = 3, mean = mu_oz, sd = sigma_oz)))
    
  }
  
  # Log-prior de mu
  log_prior_mu_oz <- dnorm(mu_oz , mean = 0, sd = 1, log = TRUE)
  
  # Log-prior de sigma (transformado) - Incluyendo el Jacobiano
  log_prior_sigma_oz <- dgamma(sigma_oz, shape = 2, rate = 2, log = TRUE) 
  
  # Log-posterior no normalizado
  log_posterior_value <- log_likelihood_oz + log_prior_mu_oz + log_prior_sigma_oz + sigma_estrella_oz
  
  return(log_posterior_value)
}


# Función de densidad posterior en escala logarítmica (Control)
log_posterior_cont <- function(params, y_vector, normal = T) {
  # 'params' tiene longitud 4
  mu_cont <- params[1]
  sigma_estrella_cont <- params[2]
  sigma_cont <- exp(sigma_estrella_cont) # Salida sigma debe ser positiva
  
  if(normal == T){
    
    # Log-verosimilitud (asumiendo datos independientes)
    log_likelihood_cont <- sum(dnorm(y_vector, mean = mu_cont, sd = sigma_cont, log = TRUE))
    
  } else {
    # Log-verosimilitud (asumiendo datos independientes)
    log_likelihood_cont <- sum(log(scaled_t_density(y_vector, df = 3, mean = mu_cont, sd = sigma_cont)))
    
  }
  
  # Log-prior de mu
  log_prior_mu_cont <- dnorm(mu_cont , mean = 0, sd = 1, log = TRUE)
  
  # Log-prior de sigma (transformado) - Incluyendo el Jacobiano
  log_prior_sigma_cont <- dgamma(sigma_cont, shape = 2, rate = 2, log = TRUE) 
  
  # Log-posterior no normalizado
  log_posterior_value <- log_likelihood_cont + log_prior_mu_cont + log_prior_sigma_cont + sigma_estrella_cont
  
  return(log_posterior_value)
}

logp_oz <- partial(log_posterior_oz, y_vector = datos_oz$gain)

logp_cont <- partial(log_posterior_cont, y_vector = datos_control$gain)

```

En este item deberia ser prudente mostrar el vector de parametros de inicio y la matriz de varianzas y covarianzas donde la matriz de 4x4 deberia tener un valor para los mu y un valor para los sigma para que la funcion pueda converger 


Luego hay que sacar muestras con la funcion de metropolis hastings en base logaritmica para cada parametro del modelo


# OZONO NORMAL

```{r,echo=FALSE, warning=FALSE, message=FALSE}
set.seed(0398)
# Entrada parametros = mu_o, sigma_oz, mu_c, sigma_c
# matriz de varianza y covarianza
sigma <- matrix(c(1.3^2   , 0   ,
                  0     , 0.15^2 ), nrow = 2)  # Propuesto en clase 0.05^2


# Obtener muestras
muestras_theta_estrella_oz <- metropolis_hastings_log(
  logp = logp_oz,
  x = c(1.12,log(15)), # propuesto en clase 0 y log(1.5)
  n = 5000,
  sigma = sigma
)
```


```{r ,echo=FALSE, message=FALSE, fig.align = "center",  fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de mu"}
#histograma que muestra la dist a posteriori marginal de mu
graficar_distribucion(
  muestras_theta_estrella_oz[ ,1], #[1000:10000]
  x_grid = 1, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 0.5,
  color = "green", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$σ_O$"),
  ylab = NULL)
# es un histograma que representa la distribucion aposteriori marginal de mu
```




se concluye brevemente cada uno de los graficos y sus traceplot y luego realizamos las medidas de diagnostico sobre cada parametro

```{r ,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplots con sigma = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas del vector de parametros
muestras_md_log_oz <- list()
n_cadenas <- 4
sigma <- matrix(c(1.3^2   , 0   ,
                  0     , 0.15^2 ), nrow = 2) #mejores valores 1.3 y 0.15

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md_log_oz[[j]] <- metropolis_hastings_log(
    logp = logp_oz,
    x = c(1.12,log(15)),
    n = 5000,
    sigma = sigma
  )
}

# creamos un data frame para poder graficar
df_muestras_md_log <- crear_df_muestras(muestras_md_log_oz)

#dataframe con las muestras de mu
df_muestras_mu <- df_muestras_md_log |> filter(paso <= 5000)


#dataframe con las muestras de sigma 
df_muestras_sigma <- df_muestras_md_log |> filter(paso > 5000)

```



```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}

set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_mu$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```


```{r ,echo=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de sigma"}
#histograma que muestra la dist a posteriori marginal de sigma (transformamos sigma_estrella)
graficar_distribucion(
  exp(muestras_theta_estrella_oz[ ,2]), #[1000:10000]burn-in (los quema a los primeros 1000 pasos)
  x_grid = 15, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 0.4,  #igual que arriba
  color = "orange", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$σ_O$"),
  ylab = NULL)
```

```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_sigma_trp$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```

# CONTROL NORMAL

```{r,echo=FALSE, warning=FALSE, message=FALSE}
set.seed(0398)
# Entrada parametros = mu_o, sigma_oz, mu_c, sigma_c
# matriz de varianza y covarianza
sigma <- matrix(c(1.3^2   , 0   ,
                  0     , 0.15^2 ), nrow = 2)  # Propuesto en clase 0.05^2


# Obtener muestras
muestras_theta_estrella_c <- metropolis_hastings_log(
  logp = logp_cont,
  x = c(2,log(15)), # propuesto en clase 0 y log(1.5)
  n = 5000,
  sigma = sigma
)
```


```{r ,echo=FALSE, message=FALSE, fig.align = "center",  fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de mu"}
#histograma que muestra la dist a posteriori marginal de mu
graficar_distribucion(
  muestras_theta_estrella_c[ ,1], #[1000:10000]
  x_grid = 1, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 0.4,
  color = "green", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$μ_C$"),
  ylab = NULL)
# es un histograma que representa la distribucion aposteriori marginal de mu
```


se concluye brevemente cada uno de los graficos y sus traceplot y luego realizamos las medidas de diagnostico sobre cada parametro

```{r ,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplots con sigma = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas del vector de parametros
muestras_md_log_c <- list()
n_cadenas <- 4
sigma <- matrix(c(1.3^2   , 0   ,
                  0     , 0.15^2 ), nrow = 2) #mejores valores 1.3 y 0.15

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md_log_c[[j]] <- metropolis_hastings_log(
    logp = logp_cont,
    x = c(2,log(15)),
    n = 5000,
    sigma = sigma
  )
}

# creamos un data frame para poder graficar
df_muestras_md_log <- crear_df_muestras(muestras_md_log_c)

#dataframe con las muestras de mu
df_muestras_mu <- df_muestras_md_log |> filter(paso <= 5000)


#dataframe con las muestras de sigma 
df_muestras_sigma <- df_muestras_md_log |> filter(paso > 5000)

```


```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}

set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_mu$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```


```{r ,echo=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de sigma"}
#histograma que muestra la dist a posteriori marginal de sigma (transformamos sigma_estrella)
graficar_distribucion(
  exp(muestras_theta_estrella_c[ ,2]), #[1000:10000]burn-in (los quema a los primeros 1000 pasos)
  x_grid = 15, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 0.4,  #igual que arriba
  color = "orange", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$σ_C$"),
  ylab = NULL)
```

```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_sigma_trp$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```


# OZONO T STUDENT
```{r}
logp_oz_t <- partial(log_posterior_oz, y_vector = datos_oz$gain, normal = F)

logp_cont_t <- partial(log_posterior_cont, y_vector = datos_control$gain, normal = F)
```

```{r,echo=FALSE, warning=FALSE, message=FALSE}
set.seed(0398)
# Entrada parametros = mu_o, sigma_oz, mu_c, sigma_c
# matriz de varianza y covarianza
sigma <- matrix(c(1.3^2   , 0   ,
                  0     , 0.15^2 ), nrow = 2)  # Propuesto en clase 0.05^2


# Obtener muestras
muestras_theta_estrella_oz <- metropolis_hastings_log(
  logp = logp_oz_t,
  x = c(1.12,log(15)), # propuesto en clase 0 y log(1.5)
  n = 5000,
  sigma = sigma
)
```


```{r ,echo=FALSE, message=FALSE, fig.align = "center",  fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de mu"}
#histograma que muestra la dist a posteriori marginal de mu
graficar_distribucion(
  muestras_theta_estrella_oz[ ,1], #[1000:10000]
  x_grid = 1, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 0.5,
  color = "green", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$μ_O$"),
  ylab = NULL)
# es un histograma que representa la distribucion aposteriori marginal de mu
```


```{r ,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplots con sigma = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas del vector de parametros
muestras_md_log_oz <- list()
n_cadenas <- 4
sigma <- matrix(c(1.3^2   , 0   ,
                  0     , 0.15^2 ), nrow = 2) #mejores valores 1.3 y 0.15

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md_log_oz[[j]] <- metropolis_hastings_log(
    logp = logp_oz_t,
    x = c(1.12,log(15)),
    n = 5000,
    sigma = sigma
  )
}

# creamos un data frame para poder graficar
df_muestras_md_log <- crear_df_muestras(muestras_md_log_oz)

#dataframe con las muestras de mu
df_muestras_mu <- df_muestras_md_log |> filter(paso <= 5000)


#dataframe con las muestras de sigma 
df_muestras_sigma <- df_muestras_md_log |> filter(paso > 5000)

```



```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}

set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_mu$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```


```{r ,echo=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de sigma"}
#histograma que muestra la dist a posteriori marginal de sigma (transformamos sigma_estrella)
graficar_distribucion(
  exp(muestras_theta_estrella_oz[ ,2]), #[1000:10000]burn-in (los quema a los primeros 1000 pasos)
  x_grid = 7, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 0.4,  #igual que arriba
  color = "orange", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$σ_O$"),
  ylab = NULL)
```


```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_sigma_trp$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```

# CONTROL T STUDEN

```{r,echo=FALSE, warning=FALSE, message=FALSE}
set.seed(0398)
# Entrada parametros = mu_o, sigma_oz, mu_c, sigma_c
# matriz de varianza y covarianza
sigma <- matrix(c(1.3^2   , 0   ,
                  0     , 0.15^2 ), nrow = 2)  # Propuesto en clase 0.05^2


# Obtener muestras
muestras_theta_estrella_c <- metropolis_hastings_log(
  logp = logp_cont_t,
  x = c(2,log(15)), # propuesto en clase 0 y log(1.5)
  n = 5000,
  sigma = sigma
)
```


```{r ,echo=FALSE, message=FALSE, fig.align = "center",  fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de mu"}
#histograma que muestra la dist a posteriori marginal de mu
graficar_distribucion(
  muestras_theta_estrella_c[ ,1], #[1000:10000]
  x_grid = 1, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 0.4,
  color = "green", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$μ_C$"),
  ylab = NULL)
# es un histograma que representa la distribucion aposteriori marginal de mu
```


```{r ,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplots con sigma = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas del vector de parametros
muestras_md_log_c <- list()
n_cadenas <- 4
sigma <- matrix(c(1.3^2   , 0   ,
                  0     , 0.15^2 ), nrow = 2) #mejores valores 1.3 y 0.15

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md_log_c[[j]] <- metropolis_hastings_log(
    logp = logp_cont_t,
    x = c(2,log(15)),
    n = 5000,
    sigma = sigma
  )
}

# creamos un data frame para poder graficar
df_muestras_md_log <- crear_df_muestras(muestras_md_log_c)

#dataframe con las muestras de mu
df_muestras_mu <- df_muestras_md_log |> filter(paso <= 5000)


#dataframe con las muestras de sigma 
df_muestras_sigma <- df_muestras_md_log |> filter(paso > 5000)

```


```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}

set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_mu$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```


```{r ,echo=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8, warning=FALSE, fig.cap="Histograma de la muestras de sigma"}
#histograma que muestra la dist a posteriori marginal de sigma (transformamos sigma_estrella)
graficar_distribucion(
  exp(muestras_theta_estrella_c[ ,2]), #[1000:10000]burn-in (los quema a los primeros 1000 pasos)
  x_grid = 15, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 0.4,  #igual que arriba
  color = "orange", #cambiar gama de colores para los parametros mu y sigma
  xlab = TeX("$σ_C$"),
  ylab = NULL)
```

```{r ,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 4, fig.height = 2.8}
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_sigma_trp$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```