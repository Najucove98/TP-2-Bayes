---
title: ""
author: ""
date: ""
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
    toc: false
    number_sections: false
    keep_tex: true
header-includes:
  - \renewcommand{\figurename}{Figura}
  - \renewcommand{\tablename}{Tabla}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{titling}
  - \usepackage{lipsum}
  - \usepackage{fancyhdr}
  - \usepackage{etoolbox}
  - \usepackage{setspace}
  - \usepackage{titlesec}
  - \usepackage{emptypage}
  - \pagestyle{fancy}
  - \usepackage{placeins}
  - \fancyhf{}
  - \patchcmd{\maketitle}{\@maketitle}{\centering\vspace*{4cm}\@maketitle}{}{}
  - \thispagestyle{empty}
editor_options: 
  markdown: 
    wrap: 72
---

```{=tex}
\begin{titlepage}
\centering
\vspace*{4cm} % espacio superior

{\Huge \textbf{Trabajo práctico 2 Estadística Bayesiana}}\\[2cm]

{\Large Agustina Roura}\\[0.5cm]
{\Large Cristian Nahuel Coveñas}\\[0.5cm]
{\Large Juan Sebastian Reines}\\[2cm]

{\large Fecha: Mayo 2025}

\vfill

\end{titlepage}
```
\newpage

# Introducción

El ozono $(O_3)$, una molécula de tres átomos de oxígeno, exhibe una
fascinante dualidad en su comportamiento y efectos. En las capas
superiores de la atmósfera, conforma la vital capa de ozono, un escudo
protector que filtra la radiación ultravioleta del sol, permitiendo la
vida tal como la conocemos. Sin embargo, a nivel de la superficie
terrestre, este mismo gas se transforma en un contaminante atmosférico
de considerable impacto negativo, afectando la salud humana a través de
la irritación del sistema respiratorio y ocular, el desencadenamiento de
episodios de asma y el deterioro de la función pulmonar. Además, su
presencia en el aire urbano contribuye al smog y puede causar daños
significativos en la vegetación.

Ante la creciente preocupación por los efectos del ozono a nivel del
suelo, Brian Tarkington, investigador del California Primate Research
Center, llevó a cabo un estudio con el objetivo de cuantificar su
impacto en el desarrollo de organismos vivos. Específicamente, se
propuso investigar si la exposición a un ambiente enriquecido con ozono
afectaba el crecimiento de ratas jóvenes. Para ello, diseñó un
experimento controlado donde un grupo de 46 ratas de la misma edad
fueron divididas aleatoriamente en dos subgrupos iguales. Uno de estos
grupos fue expuesto a un ambiente con alta concentración de ozono,
mientras que el otro se mantuvo en un ambiente libre de este gas. Tras
un período de siete días, se registró la diferencia en el peso de cada
animal, generando un conjunto de datos que permite analizar
comparativamente el efecto de la exposición al ozono.

El presente informe se centra en el análisis de estos datos utilizando
la metodología de la Estadística Bayesiana. En particular, se explorarán
dos modelos probabilísticos con sutiles diferencias en su formulación.
El primer modelo, basado en la distribución normal, con medias
$(\mu_O,\ \mu_C)$ y varianzas $(\sigma_O^2,\ \sigma_C^2)$ específicas,
representa un enfoque clásico y ampliamente utilizado en el análisis
estadístico. El segundo modelo, en contraste, emplea la distribución T
de Student con grados de libertad $(v = 3)$ para capturar posibles colas
más pesadas en la distribución de los datos. La comparación de los
resultados obtenidos bajo ambos modelos permitirá no solo responder a la
pregunta de investigación planteada por Tarkington, sino también
profundizar en la sensibilidad de las conclusiones a las asunciones
distribucionales realizadas.

Más allá de la comparación de estos modelos y la respuesta a la pregunta
de investigación sobre el efecto del ozono, el objetivo principal del
estudio es profundizar el uso del algoritmo de Metropolis-Hastings (MH)
como un método de la inferencia bayesiana. Este algoritmo se vuelve
esencial cuando la distribución de probabilidad a posteriori de los
parámetros de interés no tiene una forma analítica conocida, lo que
dificulta la obtención directa de medidas de resúmenes o la realización
de inferencias. En esencia, el algoritmo de Metropolis-Hastings funciona
como un proceso iterativo que genera una secuencia de valores (una
cadena de Markov) que, bajo ciertas condiciones, converge a la
distribución a posteriori deseada. Partiendo de un valor inicial para
los parámetros, el algoritmo sugiere un nuevo valor $propuesto$ basado
en una distribución conocida. Este nuevo valor es luego aceptado o
rechazado según una probabilidad $\alpha$ que depende de la razón entre
la verosimilitud de los datos bajo el nuevo valor de los parámetros y la
verosimilitud bajo el valor actual, multiplicada por la razón de las
probabilidades a priori de los dos valores.

$\alpha = \min \left\{ 1, \frac{p(y')q(y^{(t)} | y')}{p(y^{(t)})q(y' | y^{(t)})} \right\}$

Si el valor $propuesto$ tiene una mayor probabilidad a posteriori (o una
probabilidad no mucho menor), es más probable que sea aceptado. Si es
rechazado, el valor actual se mantiene para la siguiente iteración. Tras
un número suficiente de iteraciones, las muestras generadas por el
algoritmo se aproxima a la distribución del $posterior$ objetivo y asi
calcular diversas cantidades de interés, como medias, medianas,
intervalos de credibilidad, etc. La clave del algoritmo radica en la
elección adecuada de la distribución de propuesta para asegurar una
exploración eficiente del espacio de parámetros y una rápida
convergencia a la distribución objetivo.

\newpage

# Modelización Estadística

Para dar respuesta a la pregunta planteada por Tarkington respecto al
efecto del ozono en el crecimiento de ratas, recurrimos a modelos
bayesianos, se consideraron dos modelos probabilísticos similares pero
con diferencias sutiles en su estructura. Esto nos permite no solo
evaluar el efecto del ozono sobre el crecimiento de las ratas, sino
también explorar cómo distintos supuestos sobre la distribución de los
datos pueden influir en los resultados inferenciales.

## Modelo Normal

El primer enfoque se basa en el tradicional modelo normal, ampliamente
utilizado en el análisis estadístico. En este modelo, se asume que el
cambio de peso en las ratas sigue una distribución normal, con medias y
varianzas específicas para cada grupo (grupo expuesto al ozono y grupo
control).

$$Y_{O,i} \sim Normal(\mu_O, \sigma_O^2) \quad i = 1,...,N_O$$
$$Y_{C,j} \sim Normal(\mu_C, \sigma_C^2) \quad j = 1,...,N_C$$
$$\mu_O, \mu_C \sim Normal(0, 5^2)$$
$$\sigma_O^2, \sigma_C^2 \sim Gamma(\alpha = 6, \beta = 2)$$

donde $Y_{O,i}$ representa el cambio de peso en la i-ésima rata del
grupo expuesto al ozono, y $Y_{C,j}$ representa el cambio
correspondiente en la j-ésima rata del grupo control.

## Modelo T de Student

El segundo modelo la distribución de los datos se modela mediante una
distribución T de Student con 3 grados de libertad. Esta alternativa
resulta especialmente útil cuando se desea robustez frente a posibles
valores atípicos o colas pesadas en la distribución de los datos.
$$Y_{O,i} \sim Studen-T(\mu_O, \sigma_O^2) \quad i = 1,...,N_O$$
$$Y_{C,j} \sim Studen-T(\mu_C, \sigma_C^2) \quad j = 1,...,N_C$$
$$\mu_O, \mu_C \sim Normal(0, 5^2)$$
$$\sigma_O^2, \sigma_C^2 \sim Gamma(\alpha = 6, \beta = 2)$$

\newpage

## Índice

1.[**En el simulador: ensayos con la distribución Gamma**](#Objetivo-1)

2.[**Free practice: múltiples dimensiones**](#Objetivo-2)

3.[**El Gran Prix: ¿qué le decimos a Brian?**](#Objetivo-3)

\newpage

```{r , echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(purrr)
library(latex2exp)

# Funciones estandar para graficar
graficar_distribucion <- function(x_muestras, x_grid, p_grid, xlab = "x" ,ylab = "y", color = "grey60", color_l = "#3b78b0") {
  # Graficar histograma de las muestras con la curva de densidad teórica superpuesta.
  plt <- ggplot() +
    geom_histogram(
      aes(x = x, y = after_stat(density)),
      fill = color,     #cambie
      alpha = 0.5,      #agregue
      color = "white",  #agregue
      linewidth = 0.1,  #agregue
      bins = 50,
      data = data.frame(x = x_muestras)
    ) +
    geom_line(
      aes(x = x, y = y),
      color = color_l,  #cambie
      linewidth = 0.8, #cambie
      data = data.frame(x = x_grid, y = p_grid)
    ) +
    labs(y = ylab,      #cambie
       x= xlab) +       #agregue
    theme_bw() +
    theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(), #agregue
      axis.ticks.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none"
    )
  return(plt)
}


graficar_trazas_md <- function(df_muestras, ylab = "x" , colors = c("#107591", "#00c0bf", "#f69a48", "#fdcd49") ) {
  # Generar un _traceplot_ para 4 cadena de Markov.
  plt <- ggplot(df_muestras) +
    geom_line(aes(x = paso, y = valor, group = cadena, color = cadena), linewidth = 0.2) +
    scale_color_manual(values = colors) +
    labs(x = "Paso", y = ylab) +
    theme_bw()
  theme(
    panel.grid.minor = element_blank()
  )
  
  return(plt)
}

graficar_traza <- plot_trace <- function(x_muestras, color = "grey30", ylab = "x") {
  # Generar un _traceplot_ para una cadena de Markov.
  plt <- data.frame(x = seq_along(x_muestras), y = x_muestras) |>
    ggplot() +
    geom_line(aes(x = x, y = y), color = color , alpha = 0.8, linewidth = 0.2) + #agregue alpha
    labs(x = "Paso", y = ylab) + #cambie
    theme_bw()
  theme(
    panel.grid.major = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    legend.position = "none"
  )
  return(plt)
}

# Traemos las funciones que vamos a utilizar para el método de Metropolis-Hastings
metropolis_hastings_normal <- function(n, x, p, sigma, verbose = FALSE) {
  # Algortimo de Metropolis Hastings en una dimensión.
  # La distribución de propuesta es normal.
  # Parámetros
  #  ------------------------------------------------------------------------
  #  n         -Cantidad de muestras a obtener.                             
  #  x         -Posición inicial del algoritmo.                             
  #  p         -Función de densidad objetivo, normalizada o sin normalizar. 
  #  sigma     -Desvío estándar de la distribución de propuesta normal.     
  #  verbose   -Indica si se muestran mensajes con información del          
  #             algoritmo en cada paso.                                     
  #  ------------------------------------------------------------------------
  
  muestras <- numeric(n)
  muestras[1] <- x
  
  # Iterar desde i = 1 hasta i = n - 1
  for (i in seq_len(n - 1)) {
    # Paso 1: Proponer un nuevo valor
    x_actual <- muestras[i]
    x_propuesto <- rnorm(1, mean = x_actual, sd = sigma)
    
    # Paso 2: Calcular probabilidad de aceptación
    p_actual <- p(x_actual)
    p_propuesto <- p(x_propuesto)
    
    # Corrección en base a la densidad de la distribución de propuesta
    q_propuesto <- dnorm(x_propuesto, mean = x_actual, sd = sigma)  # q(x_propuesto | x_actual)
    q_actual <- dnorm(x_actual, mean = x_propuesto, sd = sigma)     # q(x_actual | x_propuesto)
    
    alpha <- min(1, (p_propuesto / p_actual) * (q_actual / q_propuesto))
    
    # Paso 3: Dedicir si se acepta el valor propuesto
    u <- runif(1)
    aceptar <- u < alpha
    
    # Guardar posición
    if (aceptar) {
      muestras[i + 1] <- x_propuesto
    } else {
      muestras[i + 1] <- x_actual
    }
    
    # Si 'verbose' es TRUE, mostrar valores de variables relevantes
    if (verbose) {
      cat("-----------------------\n")
      cat("x_actual: ", x_actual, "x_propuesto", x_propuesto, "\n")
      cat("p_actual: ", p_actual, "p_propuesto", p_propuesto, "\n")
      cat("q_propuesto: ", q_propuesto, "q_actual", q_actual, "\n")
      cat("alpha:", alpha, "u:", u, "aceptar:", aceptar, "\n")
    }
  }
  
  return(muestras)
}

crear_df_muestras <- function(muestras) {
  # Muestras es una lista de vectores.
  # Hay tantos vectores como cadenas.
  
  n_cadenas <- length(muestras)
  n_muestras <- length(muestras[[1]])
  
  df <- data.frame(
    cadena = as.factor(rep(seq_len(n_cadenas), each = n_muestras)),
    paso = rep(seq_len(n_muestras), n_cadenas),
    valor = unlist(muestras)
  )
  return(df)
}

```

# Metropolis-Hastings en espacios paramétricos acotados {#Objetivo-1}

El algoritmo de Metropolis-Hastings, implementado en escala logarítmica,
constituye una herramienta robusta para obtener muestras del
$posterior$. Sin embargo, su aplicación directa a modelos complejos
puede ser desafiante sin una sólida comprensión de sus fundamentos y
técnicas asociadas. Como preparación para la aplicación del algoritmo de
Metropolis-Hastings a los modelos bayesianos propuestos en este trabajo,
se llevó a cabo una exploración inicial del algoritmo en escenarios más
simples. Se comenzó con un escenario donde $X$ es la variable aleatoria
de interés, cuya función de densidad corresponde a una distribución
Gamma con parámetros $\alpha = 3$ y $\beta = 2$ ($X \sim Gamma(3,2)$).

```{r f1,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Distribucion de densidad de X" }
set.seed(0398)
# Cremos una grilla de valores de x:
# X está en los reales positivos
grid_x <- seq(0, 6, length.out = 500)

# Creamos la grilla de las p_x
grid_px <- dgamma(grid_x, 3, 2)

# Graficamos la funcion de densidad de la X
ggplot(data.frame(x = grid_x, y = grid_px), aes(x = x, y = y)) +
  geom_area(fill = "#3b78b0", alpha = 0.5) +
  geom_line(linewidth = 0.8, color = "#3b78b0") +
  labs(y = TeX("$f_X(x)$")) + 
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
  )
```

La Figura @ref(fig:f1) ilustra la forma de la distribución de densidad
de la variable aleatoria $X$.

A continuación, se propone encontrar la función de densidad de una nueva
variable aleatoria $Y=log(X)$. Para ello, recurrimos a la técnica de
transformación de variables de la siguiente manera:

$\text{Sea }Y = g(X) = log(x) \text{, donde la función } g:(0,\ +\infty) \rightarrow \mathbb{R}.$

$\text{Hallamos la función inversa: }g^{-1}(X) = e^{X} \text{, con dominio } g^{-1}: \mathbb{R} \rightarrow (0,\ +\infty).$

Luego, la función de densidad de $Y$, $P_Y(y)$, se obtiene a partir de
la función de densidad $X$, $P_X(x)$, utilizando la siguiente relación:

$$P_Y(y) = P_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$$
$\text{Sustituyendo } g^{-1}(y) = e^{y}, \text{ obtenemos:}$

$$P_Y(y) = P_X(e^{y}) \left| \frac{d}{dy} e^{y} \right| = P_X(e^{y})\left| e^{y} \right|$$

$\text{De esta forma podemos graficar la función de densidad de } Y = log(X)$

```{r f2,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Distribucion de densidad de Y = log(X)"}
set.seed(0398)
# Creamos una funcion:
py <- function(y){
  dgamma(exp(y), shape = 3, rate = 2) * exp(y) 
}

# Creamos una grilla de valores de y:
# Y está en los reales
grid_y <- seq(-4, 3, length.out = 10000)

# Creamos la grilla de las p_y
grid_p_y <- py(grid_y)

# Graficamos la funcion de densidad de Y
ggplot(data.frame(x = grid_y, y = grid_p_y), aes(x = x, y = y))  +
  geom_area(fill = "#44A699", alpha = 0.5) +
  geom_line(linewidth = 0.8, color = "#44A699") +
  labs(y = TeX("$f_Y(y)$"),
       x= "y") + 
  scale_x_continuous(expand = expansion(mult = c(0, 0.05))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
  )
```

Dadas las funciones de densidad para las variables $X$ e $Y=log(X)$, tal
como se ilustra en las Figuras @ref(fig:f1) y @ref(fig:f2)
respectivamente, se consideró la aplicación del algoritmo de
Metropolis-Hastings para generar 10000 muestras de $X$. Si bien la
distribución normal suele ser una elección inicial conveniente para
generar propuestas de nuevos valores en el algoritmo de
Metropolis-Hastings, su aplicación en espacios paramétricos acotados
puede resultar ineficiente debido al incremento en la tasa de rechazo
generado por propuestas fuera del dominio, lo que ralentiza la
exploración del espacio paramétrico y la convergencia del algoritmo. Por
lo que se optó por realizar el proceso de muestreo sobre la variable
transformada $Y=log(X)$, utilizando una distribución normal como función
de propuesta, con una desviación estándar $\sigma = 0.2$ y centrada en
un valor inicial de $\mu = 0$. Esta estrategia de trabajar en un espacio
no acotado permitió seguir utilizando las propiedades convenientes de la
distribución normal como propuesta.

```{r f3,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Histograma de la muestras de Y"}
set.seed(0398)
# Sacamos muestras:
# Hacemos un nuevo grid_py:
grid_y <- seq(-4, 3, length.out = 10000)

muestra <- metropolis_hastings_normal(
  n = 10000,
  x = 0,
  sigma = 0.2,
  p = py
  )

# Graficando:
graficar_distribucion(muestra, 
                      # En el eje x van los grid_py(osea los valores 
                      # "simulados" de y)
                      x_grid = grid_y, 
                      # En el eje y van los valores de P(y) 
                      p_grid = py(grid_y),
                      xlab = "y",
                      ylab = TeX("$f_Y(y)$"),
                      color = "#44A699",
                      color_l = "#44A699")
```

```{r f4,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplot de la muestras de Y"}
graficar_traza(muestra, ylab = "y", color = "#44A699" )
```

Tras la implementación del método, y tal como se evidencia en la Figura
@ref(fig:f3), las muestras obtenidas presentan una semejanza notable con
la distribución objetivo. Además, la Figura @ref(fig:f4) (trace plot)
revela una baja autocorrelación en las cadenas de Markov generadas.Esto
sugiere un buen ajuste del algoritmo de Metropolis-Hastings, resultado
de su aplicación sobre la variable aleatoria Y, cuyo dominio resultó
apropiado para la distribución de propuesta utilizada. Para una
evaluación más exhaustiva de la calidad del ajuste del método
implementado, se calcularon las siguientes medidas de diagnóstico.

```{r,echo=FALSE, warning=FALSE, message=FALSE}

# Funciones para medidas de diagnostico
# Diagnóstico R de Gelman-Rubin
calcular_W <- function(muestras) {
    # muestras: matriz de dimensión (n_muestras, n_cadenas)
    return(mean(apply(muestras, 2, var)))
}

calcular_B <- function(muestras) {
    # muestras: matriz de dimensión (n_muestras, n_cadenas)
    S <- nrow(muestras) # cantidad de muestras
    M <- ncol(muestras) # cantidad de cadenas
    media_cadenas <- apply(muestras, 2, mean)
    media_global <- mean(media_cadenas)
    diferencias_cuadrado <- (media_cadenas - media_global) ^ 2
    return(S / (M - 1) * sum(diferencias_cuadrado))
}

calcular_R <- function(muestras) {
    # muestras: matriz de dimensión (n_muestras, n_cadenas)
    S <- nrow(muestras)
    W <- calcular_W(muestras)
    B <- calcular_B(muestras)
    numerador <- ((S - 1) / S) * W + B / S
    return(sqrt(numerador / W))
}

# Tamaño efectivo de muestra N_eff
calcular_n_eff <- function(muestras) {
    # muestras: vector de longitud 'n_muestras'
    autocorrelacion <- acf(muestras, lag = Inf, plot = FALSE)$acf
    limite <- which(autocorrelacion < 0.05)[1]
    numerador <- length(muestras)
    denominador <- 1 + 2 * sum(autocorrelacion[2:limite])
    return(numerador / denominador)
}

```

```{r f5,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplots con σ = 0.2"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas de muestras con el metodo usado y una propuesta normal con sigma 0.2
muestras_md <- list()
n_muestras <- 10000
n_cadenas <- 4
x_inicio <- 0
sigma <- 0.2

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md[[j]] <- metropolis_hastings_normal(
    n = n_muestras,
    x = x_inicio,
    p = py,
    sigma = sigma
  )
}

#Con esto descartamos las primeras mil muestras que aun "no convergen"
#df_muestras <- crear_df_muestras(muestras) |> filter(paso > 1000)


#necesario para crear el traceplot
df_muestras_md <- crear_df_muestras(muestras_md)

# Grafica las 4 trazas
graficar_trazas_md(df_muestras_md, ylab = "y", colors = c("#AEC6CF", "#F08080", "#FAEBD7", "#DDA0DD"))

```

```{r f6,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5}
library(knitr)
library(kableExtra)
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_md$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

library(kableExtra)
# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")

#En el traceplot, las cadenas se mueven lentamente, lo que indica una alta autocorrelación. 
#Además, no se mezclan y tampoco parecen converger a ninguna distribución objetivo. 
#El valor de es considerablemente mayor a 1, lo que también señala falta de convergencia y
#resulta consistente con lo observado en el traceplot. Además, el tamaño efectivo de muestra 
#en cada cadena es muy bajo, y en consecuencia, el tamaño efectivo de muestra total también 
#lo es, lo cual es coherente con cadenas que presentan una autocorrelación muy alta.

#Así, se concluye que no se pueden utilizar las muestras para realizar inferencia. 
#Decidimos incrementar el desvío estándar de la distribución de propuesta.
```


```{r f7,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplots con σ = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas de muestras con el metodo usado y una propuesta normal con sigma 0.2
muestras_md <- list()
n_muestras <- 10000
n_cadenas <- 4
x_inicio <- 0
sigma <- 1.3

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md[[j]] <- metropolis_hastings_normal(
    n = n_muestras,
    x = x_inicio,
    p = py,
    sigma = sigma
  )
}

# Con esto descartamos las primeras mil muestras que aun "no convergen"
df_muestras_md <- crear_df_muestras(muestras_md)



# Grafica las 4 trazas
graficar_trazas_md(df_muestras_md, ylab = "y", colors = c("#87CEEB", "#FFA07A", "#FFFFE0", "#B0E0E6"))
```

```{r f8,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5}
set.seed(0398)
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_md$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

library(kableExtra)
# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```



```{r f9,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Histograma de la muestras de Y"}
set.seed(2198)
# Copiando lo del profe:
muestras <- metropolis_hastings_normal(
  n = 10000,
  x = 0,
  p = py ,
  sigma = 1.3
)

# grafica la muestra de Y con una  propuesta de sigma = 1.3
graficar_distribucion(muestras, grid_y, py(grid_y), xlab = "y", ylab = TeX("$f_Y(y)$"), color = "#44A699", color_l = "#44A699")
```
```{r f10,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplot de la muestras de Y"}
# grafica la traza de la transformada
graficar_traza(muestras, color = "#44A699", ylab = "y")
```


```{r f11,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Histograma de la muestras de X"}
# Transformamos las muestras de Y a muestras de X:
muestras_x <- exp(muestras)

# histograma de la muestra objetivo
graficar_distribucion(muestras_x, grid_x, grid_px , ylab = TeX("$f_X(x)$"), color = "#3b78b0", color_l = "#3b78b0")
```

```{r f12,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplot de la muestras de X"}
# Traza de la muestra objetivo
graficar_traza(muestras_x, color = "#3b78b0")
```

```{r,echo=FALSE, warning=FALSE, message=FALSE}
# Completamos el codigo:
metropolis_hastings_log <- function(logp, x, n, sigma = NULL) {
  # Algortimo de Metropolis Hastings en escala logarítmica
  #
  # Parámetros
  #  ------------------------------------------------------------------------
  #  logp      Función de densidad objetivo, normalizada o sin normalizar, |
  #            en escala logarítmica.                                      |
  #  x         Posición inicial del algoritmo.                             |
  #  n         Cantidad de muestras a obtener.                             |
  #  sigma     Matriz de varianzas y covarianzas para la distribución de   |
  #            propuesta. Por defecto, es NULL y usa la matriz identidad.  |
  #  ------------------------------------------------------------------------
  
  # Obtener dimensionalidad de la distribución objetivo a partir del punto inicial
  p <- length(x)
  
  # Inicializar matriz donde se guardan las muestras
  muestras <- matrix(NA, nrow = n, ncol = p)
  
  # Usar matriz diagonal unitaria para la distribución de propuesta cuando no se especifica
  if (is.null(sigma)) {
    sigma <- diag(p)
  }
  
  # Almacenar el punto inicial en la matriz de muestras
  muestras[1, ] <- x
  
  for (i in 1:(n - 1)) {
    # Obtener el valor de la muestra actual
    muestra_actual <- muestras[i, ]
    
    # Proponer un nuevo valor
    muestra_propuesta <- mvtnorm::rmvnorm(1, mean = muestra_actual, sigma = sigma)
    
    # Evaluar la log densidad en el valor actual y en el propuesto
    logp_propuesta <- logp(muestra_propuesta)
    logp_actual <- logp(muestra_actual)
    
    # Log densidad al pasar de muestra_propuesta a muestra_actual y viceversa
    logq_actual <- mvtnorm::dmvnorm(
      muestra_actual, mean = muestra_propuesta, sigma = sigma, log = TRUE
    )
    logq_propuesta <- mvtnorm::dmvnorm(
      muestra_propuesta, mean = muestra_actual, sigma = sigma, log = TRUE
    )
    
    # Calcular log probabilidad de aceptación
    # Es mas estable con sumas y restas
    log_alpha <- (
      (logp_propuesta - logp_actual) + (logq_actual - logq_propuesta)
    )
    
    # Simular aceptación o rechazo
    # Opcion 1: comparacion en escala logarítmica
    log_u <- log(runif(1))
    
    aceptar <- log_u < log_alpha
    
    # Opcion 2: comparacion en escala original
    # u <- runif(1)
    # aceptar <- u < exp(log_alpha)
    
    # Determinar siguiente paso en base al criterio de selección
    if (aceptar) {
      muestras[i + 1, ] <- muestra_propuesta
    } else {
      muestras[i + 1, ] <- muestra_actual
    }
  }
  # Convertir 'muestras' a vector si se trata de una distrbución univariada
  if (p == 1) {
    muestras <- as.vector(muestras)
  }
  
  return(muestras)
}
```


```{r,echo=FALSE, warning=FALSE, message=FALSE}
set.seed(2198)
# Funcion en escala logaritmica
logp_y <- function(y){
  # Equivalente a log(dgamma(x, shape = 3, rate = 2))
  # Pero es más estable:
  dgamma(exp(y), shape = 3, rate = 2, log = T) + y
}


muestralog <- metropolis_hastings_log(
  logp = logp_y,
  x = 0, 
  n = 10000, 
  # Como usamos la normal multivariada, no usamos desvios o variancias
  # usamos la matriz de covariancias, pero en los elementos diagonales
  # tiene las variancias, no los desvíos, por eso se eleva al cuadrado
  sigma = as.matrix(1.3^2)
)
```


```{r f13,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Histograma de la muestras de Y"}
graficar_distribucion(muestralog, grid_y, grid_p_y, xlab = "y", ylab = TeX("$f_Y(y)$"), color = "#44A699", color_l = "#44A699")
```


```{r f14,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplot de la muestras de Y"}
graficar_traza(muestralog, color = "#44A699", ylab = "y")
```


```{r f15,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplots con σ = 1.3"}
set.seed(0398)
# Creamos una lista para guardar 4 cadenas de muestras con el metodo usado y una propuesta normal con sigma 0.2
muestras_md_log <- list()
n_muestras <- 10000
n_cadenas <- 4
x_inicio <- 0
sigma <- as.matrix(1.3^2)

# Se llama a la función `metropolis_hastings_normal` y se guardan los resultados en una lista.
for (j in seq_len(n_cadenas)) {
  muestras_md_log[[j]] <- metropolis_hastings_log(
    n = n_muestras,
    x = x_inicio,
    logp = logp_y,
    sigma = sigma
  )
}

# Con esto descartamos las primeras mil muestras que aun "no convergen"
df_muestras_md_log <- crear_df_muestras(muestras_md_log)

# Grafica las 4 trazas
graficar_trazas_md(df_muestras_md_log, ylab = "y", colors = c("#87CEEB", "#FFA07A", "#FFFFE0", "#B0E0E6"))
```

```{r f16,echo=FALSE, warning=FALSE, message=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5}
# se juntan las cadenas en una matriz para obtener las medidas de diagnostico
muestras_matriz <- matrix(df_muestras_md_log$valor, ncol = n_cadenas, byrow = FALSE)


# Calculamos las medidas de diagnostico

R_hat <- calcular_R(muestras_matriz)
n_effs <- apply(muestras_matriz, 2, calcular_n_eff) # Para cada columna, calcular N_eff
n_eff <- sum(n_effs) # Sumar los N_eff de todas las cadenas

library(kableExtra)
# Crear un data frame para la tabla
tabla_diagnostico <- data.frame(
  Métrica = c("$\\hat{R}$", paste0("$N_{eff}$ Cadena ", 1:length(n_effs)), "$N_{eff}$ Total"),
  Valor = c(R_hat, n_effs, n_eff)
)

# Imprimir la tabla usando kable con opciones de kableExtra
kable(tabla_diagnostico,
      col.names = c("Métrica", "Valor"),
      caption = "Medidas de Diagnóstico del Algoritmo Metropolis-Hastings",
      escape = FALSE,
      booktabs = TRUE,
      position = "H",
      caption.placement = "bottom") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(2, width = "3cm")
```

```{r f17,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Histograma de la muestras de X"}
# Muestras de X transformando la muestra de Y
muestras_x <- exp(muestralog)
graficar_distribucion(muestras_x, grid_x, grid_px, ylab = TeX("$f_X(x)$"), color = "#3b78b0", color_l = "#3b78b0")
```


```{r f18,echo=FALSE, fig.align = "center", fig.width = 5, fig.height = 3.5, warning=FALSE, fig.cap="Traceplot de la muestras de Y"}
graficar_traza(muestras_x, color = "#3b78b0")                           
```


### Free practice: múltiples dimensiones {#Objetivo-2}

En este escenario se le agrega que la distribución objetivo tiene múltiples dimensiones y debe ser obtenida mediante el uso de la regla de Bayes.

Dado el siguiente modelo:

$$Y_i \sim \text{Normal}(\mu, \sigma^2) \text{, con } i=1,...,N$$
$$\mu \sim \text{Normal}(0, 1^2) $$
$$\sigma \sim \text{Gamma}(\alpha = 2, \beta = 2)$$
Una forma convencional de obtener el posterior no normalizado es usando la regla de bayes

$$p(\boldsymbol{\theta} \text{ | } \boldsymbol{y} ) = \dfrac{p( \boldsymbol{y}  \text{ | } \boldsymbol{\theta}). p(\boldsymbol{\theta})}{p(\boldsymbol{y})} $$
$$p(\mu, \ \sigma \text{ | } \boldsymbol{y} ) = \dfrac{p( \boldsymbol{y}  \text{ | } \mu, \ \sigma).p(\mu, \ \sigma)}{p(\boldsymbol{y})} \propto p( \boldsymbol{y}  \text{ | } \mu, \ \sigma).p(\mu, \ \sigma)$$
$$\text{sabiendo que }\mu \text{ y } \sigma \text{ son independientes se tiene: } $$
$$p(\mu, \ \sigma \text{ | } \boldsymbol{y} ) \propto p( \boldsymbol{y}  \text{ | } \mu, \ \sigma).p(\mu).p(\sigma)$$
Un posible problema para esa deduccion es el dominio de $\boldsymbol{\theta} = \{ \mu,\ \sigma \}\ \implies \boldsymbol{\theta} \in \mathbb{R} \text{ x } \mathbb{R}^+$
Esto se soluciona haciendo una transformacion al parametro acotado de la forma $\boldsymbol{\theta^*} = \{ \mu,\ \sigma^* \}\ \implies \boldsymbol{\theta^*} \in \mathbb{R}^2 \ \text{con }\sigma^*=log(\sigma)$

```{r , echo=FALSE}
# paso 1) transformar los parametros tal que su dominio sea Rp
# paso 2) tener posterior no normalizado de theta* donde theta* =(mu, sigma*)
#         sigma* = log(sigma)
# paso 3) obtener el posterior no normalizado de theta* en escala logaritmica
# paso 3.5) implemetar el paso 3 en R
# paso 4) obtener muestras de theta*
# paso 5) convertir las muestras de theta* en muestras de theta
```


$$P(\mu, \sigma^* \mid y) ~ P_y(y \mid \mu, \sigma^*)P_\mu(\mu)P_\sigma^*(\sigma^*)$$

$$\text{Teniendo en cuando la transformacion de }\sigma^* \text{ tenemos:}$$
$$P_\sigma^*(\sigma^*) = P_\sigma^*(g^1 (\sigma^*)) \left| \dfrac{d}{d\sigma^*} (g^1 (\sigma^*)) \right| \ = \ P_\sigma^*(exp(\sigma^*))*\exp(\sigma^*) $$
$$\left[ \sum_{i = 1}^{N} P_y(y_i \mid \mu,\exp(\sigma^*))\right]*P_\mu(\mu)*P_\sigma^*(exp(\sigma^*))*\exp(\sigma^*)$$
Implemente en R una función que permita calcular la densidad a posteriori en escala logarítmica, sin incluir la constante de normalización. Esta función tendrá 2 parámetros de entrada, uno para el vector de parámetros y otro para el vector de valores observados.



```{r}
# Función de densidad del posterior en escala logarítmica
log_posterior <- function(params, y_vector){
   # "params" tiene longitud 2
   mu <- params[1]
   sigma_estrella <- params[2]
   # salida guarda la formula para obtener el posterior
   salida <-(
     sum(dnorm(y_vector, mean = mu, sd = exp(sigma_estrella), log = T)) # sum(logp(y_i|mu,exp(sigma*)))
     + dnorm(mu, mean = 0, sd = 1, log = T)   # logp(mu)
     + dgamma(exp(sigma_estrella),shape = 2, rate = 2, log = T) # logp(exp(sigma*))
     + sigma_estrella # sigma estrella
   ) 
   return(salida)
}
```

```{r,echo=FALSE, warning=FALSE, message=FALSE}
#ITEM 5
library(purrr)

#Se utilizo la muestra provista
y_datos <- as.numeric(readLines(
  "https://raw.githubusercontent.com/ee-unr/estadistica-bayesiana/refs/heads/main/datos/precalentamiento-mh.txt"
))

logp <- partial(log_posterior, y_vector = y_datos)
log_posterior(c(0, log(2)), y_datos)
logp(c(0,log(2)))

# paso 4
#Obtener muestras
sigma <- diag(2)* (0.05**2)

muestras_theta_estrella <- metropolis_hastings_log(
  logp = logp,
  x = c(0.5, log(1.5)), #muestriamos una distribucion multivariada pasamos dos puntos iniciales (dos dimensiones en el mundo de los parametros)
  n = 10000,
  sigma = sigma
)
dim(muestras_theta_estrella)





#Tamaño efectivo de muestra va a ser mmenor que el tamaño de nuestra

#histograma que muestra la dist a posteriori marginal de mu
graficar_distribucion(
  muestras_theta_estrella[ ,1][1000:10000],
  x_grid = 1, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 1,
  color = "green"
  )
# es un histograma que representa la distribucion aposteriori marginal de mu

#traceplot de mu
plot_trace(muestras_theta_estrella[ ,1], color = "green")

#histograma que muestra la dist a posteriori marginal de sigma (transformamos sigma_estrella)
graficar_distribucion(
  exp(muestras_theta_estrella[ ,2][1000:10000]), #burn-in (los quema a los primeros 1000 pasos)
  x_grid = 1, #No podemos graficar la linea azul porq de antemano no conocemos la distribucion
  p_grid = 1,  #igual que arriba
  color = "orange"
)

#Traceplot de sigma estrella
plot_trace(muestras_theta_estrella[ ,2], color = "orange")
```
Son muestras obtenidas a partir d ela distribucion log, cuando aun no
habian convergido




### El Gran Prix: ¿qué le decimos a Brian? {#Objetivo-3}
